{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какое количество песен будет достаточным?\n",
    "### Нужны ли цепи Маркова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Наталья\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "nltk.download('punkt')\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import KneserNeyInterpolated as KNI\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Eminem...\n",
      "\n",
      "Song 1: \"Rap God\"\n",
      "Song 2: \"Killshot\"\n",
      "Song 3: \"Lose Yourself\"\n",
      "Song 4: \"The Monster\"\n",
      "Song 5: \"Lucky You\"\n",
      "Song 6: \"Godzilla\"\n",
      "Song 7: \"The Ringer\"\n",
      "Song 8: \"River\"\n",
      "Song 9: \"Berzerk\"\n",
      "Song 10: \"Venom\"\n",
      "Song 11: \"Not Alike\"\n",
      "Song 12: \"Fall\"\n",
      "Song 13: \"Stan\"\n",
      "Song 14: \"Without Me\"\n",
      "Song 15: \"The Real Slim Shady\"\n",
      "Song 16: \"Walk on Water\"\n",
      "Song 17: \"Kamikaze\"\n",
      "Song 18: \"’Till I Collapse\"\n",
      "Song 19: \"Bad Guy\"\n",
      "Song 20: \"Love the Way You Lie\"\n",
      "Song 21: \"Mockingbird\"\n",
      "Song 22: \"8 Mile: B-Rabbit vs Papa Doc\"\n",
      "Song 23: \"Headlights\"\n",
      "Song 24: \"Survival\"\n",
      "Song 25: \"Not Afraid\"\n",
      "Song 26: \"No Love\"\n",
      "Song 27: \"Beautiful\"\n",
      "Song 28: \"Greatest\"\n",
      "Song 29: \"When I’m Gone\"\n",
      "Song 30: \"Love Game\"\n",
      "Song 31: \"My Name Is\"\n",
      "Song 32: \"Legacy\"\n",
      "Song 33: \"Cleanin’ Out My Closet\"\n",
      "Song 34: \"The Way I Am\"\n",
      "Song 35: \"Space Bound\"\n",
      "Song 36: \"Like Toy Soldiers\"\n",
      "Song 37: \"Guts Over Fear\"\n",
      "Song 38: \"Superman\"\n",
      "Song 39: \"Unaccommodating\"\n",
      "Song 40: \"Sing for the Moment\"\n",
      "Song 41: \"Stronger Than I Was\"\n",
      "Song 42: \"Believe\"\n",
      "Song 43: \"Evil Twin\"\n",
      "Song 44: \"Detroit vs. Everybody\"\n",
      "Song 45: \"Untouchable\"\n",
      "Song 46: \"Marshall Mathers\"\n",
      "Song 47: \"So Much Better\"\n",
      "Song 48: \"Kill You\"\n",
      "Song 49: \"Beautiful Pain\"\n",
      "Song 50: \"Kim\"\n",
      "Song 51: \"Campaign Speech\"\n",
      "Song 52: \"Darkness\"\n",
      "Song 53: \"Rhyme or Reason\"\n",
      "Song 54: \"Criminal\"\n",
      "Song 55: \"Stepping Stone\"\n",
      "Song 56: \"Chloraseptic (Remix)\"\n",
      "Song 57: \"I’m Back\"\n",
      "Song 58: \"Kings Never Die\"\n",
      "Song 59: \"Asshole\"\n",
      "Song 60: \"Infinite\"\n",
      "Song 61: \"Just Don’t Give a Fuck\"\n",
      "Song 62: \"Wicked Ways\"\n",
      "Song 63: \"So Far...\"\n",
      "Song 64: \"White America\"\n",
      "Song 65: \"Phenomenal\"\n",
      "Song 66: \"Good Guy\"\n",
      "Song 67: \"Normal\"\n",
      "Song 68: \"You Gon’ Learn\"\n",
      "Song 69: \"BET Shady 2.0 Cypher\"\n",
      "Song 70: \"Shake That\"\n",
      "Song 71: \"Brainless\"\n",
      "Song 72: \"Offended\"\n",
      "Song 73: \"Role Model\"\n",
      "Song 74: \"The Warning\"\n",
      "Song 75: \"Guilty Conscience\"\n",
      "Song 76: \"25 To Life\"\n",
      "Song 77: \"Premonition (Intro)\"\n",
      "Song 78: \"FACK\"\n",
      "Song 79: \"Arose\"\n",
      "Song 80: \"Framed\"\n",
      "Song 81: \"Ass Like That\"\n",
      "Song 82: \"Hailie’s Song\"\n",
      "Song 83: \"Bitch Please II\"\n",
      "Song 84: \"Those Kinda Nights\"\n",
      "Song 85: \"Just Lose It\"\n",
      "Song 86: \"Cold Wind Blows\"\n",
      "Song 87: \"Business\"\n",
      "Song 88: \"Chloraseptic\"\n",
      "Song 89: \"Bad Husband\"\n",
      "Song 90: \"Leaving Heaven\"\n",
      "Song 91: \"Brain Damage\"\n",
      "Song 92: \"Going Through Changes\"\n",
      "Song 93: \"Cinderella Man\"\n",
      "Song 94: \"ShadyXV\"\n",
      "Song 95: \"My Darling\"\n",
      "Song 96: \"Marsh\"\n",
      "Song 97: \"Drug Ballad\"\n",
      "Song 98: \"Rap God - The Fast Part\"\n",
      "Song 99: \"We Made You\"\n",
      "Song 100: \"Castle\"\n",
      "\n",
      "Reached user-specified song limit (100).\n",
      "Done. Found 100 songs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking whether installed module works correctly, setting Eminem as the author\n",
    "genius = lyricsgenius.Genius(\"jHJB8vqrbfVDIGM1KrETFx6s7EG9SGN3RGcmr63z7Gjgz8dYY4LizTL9rGupTAl1\")\n",
    "#at first used to sort by title, but it brings remixes of same song, so let us sort by popularity \n",
    "artist = genius.search_artist(\"Eminem\", max_songs=100, sort=\"popularity\")\n",
    "genius.remove_section_headers = True\n",
    "genius.verbose = False\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote `Lyrics_Eminem.json`\n"
     ]
    }
   ],
   "source": [
    "artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alternate_names', 'api_path', 'description', 'facebook_name', 'followers_count', 'header_image_url', 'id', 'image_url', 'instagram_name', 'is_meme_verified', 'is_verified', 'name', 'translation_artist', 'twitter_name', 'url', 'current_user_metadata', 'iq', 'description_annotation', 'user', 'songs'])\n"
     ]
    }
   ],
   "source": [
    "lyrics = 'Lyrics_Eminem.json'\n",
    "with open(lyrics) as eminem_lyrics:\n",
    "    messyvocab = json.load(eminem_lyrics)\n",
    "dict_keys = messyvocab.keys()\n",
    "print(dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Intro]\\n\"Look, I was gonna go easy on you not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Intro]\\nYou sound like a bitch, bitch\\nShut t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Intro]\\nLook, if you had one shot or one oppo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Song lyrics\n",
       "0  [Intro]\\n\"Look, I was gonna go easy on you not...\n",
       "1  [Intro]\\nYou sound like a bitch, bitch\\nShut t...\n",
       "2  [Intro]\\nLook, if you had one shot or one oppo..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = messyvocab.get('songs')\n",
    "lyrics_vocab = []\n",
    "for i in range(0,len(vocab)):\n",
    "    index = vocab[i]\n",
    "    song = index.get('lyrics')\n",
    "    lyrics_vocab.append(song)\n",
    "lyricsdf = pd.DataFrame(lyrics_vocab, columns=['Song lyrics'])\n",
    "lyricsdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "lyricsdf = lyricsdf.dropna()\n",
    "print(len(lyricsdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(lyricsdf)):\n",
    "    stringlyric = lyricsdf.iloc[i,0]\n",
    "    stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\n\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\\\n\", \"\\n\")\n",
    "    stringlyric.replace(\"\\\\\", \"\\n\")\n",
    "    for x in ['intro', 'verse', 'chorus','!','\"', \"'\", '’', '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','\\\\',']',\n",
    " '^','_','`','{','|','}','~', '1', '...']:   \n",
    "        stringlyric = stringlyric.replace(x, '')\n",
    "        lyricsdf.iloc[i,0] = stringlyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.models import LanguageModel\n",
    "#copypaste from NLTK source documentation\n",
    "class InterpolatedLanguageModel(LanguageModel):\n",
    "    def __init__(self, smoothing_cls, order, **kwargs):\n",
    "        assert issubclass(smoothing_cls, Smoothing)\n",
    "        params = kwargs.pop(\"params\", {})\n",
    "        super().__init__(order, **kwargs)\n",
    "        self.estimator = smoothing_cls(self.vocab, self.counts, **params)\n",
    "\n",
    "\n",
    "    def unmasked_score(self, word, context=None):\n",
    "        if not context:\n",
    "                return self.estimator.unigram_score(word)\n",
    "        if not self.counts[context]:\n",
    "             return self.unmasked_score(word, context[1:])\n",
    "        alpha, gamma = self.estimator.alpha_gamma(word, context)\n",
    "        return alpha + gamma * self.unmasked_score(word, context[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_non_zero_vals(dictionary):\n",
    "    return sum(1.0 for c in dictionary.values() if c > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is KneserNey from NLTK documentation but with some amendments from https://github.com/nltk/nltk/pull/2363/commits/ce74e449dc9526e19596b1c4a9c510bbb35812cc \n",
    "from nltk.lm.models import Smoothing\n",
    "class KneserNey(Smoothing):\n",
    "    def __init__(self, vocabulary, counter, discount=0.1, **kwargs):\n",
    "        super(KneserNey, self).__init__(vocabulary, counter, *kwargs)\n",
    "        super().__init__(vocabulary, counter, **kwargs)\n",
    "        self.discount = discount\n",
    "\n",
    "    def unigram_score(self, word):\n",
    "        return 1.0 / len(self.vocab)\n",
    "\n",
    "    def alpha_gamma(self, word, context):\n",
    "        prefix_counts = self.counts[context]\n",
    "        prefix_total_ngrams = prefix_counts.N()\n",
    "        alpha = max(prefix_counts[word] - self.discount, 0.0) / prefix_total_ngrams\n",
    "        gamma = self.discount * _count_non_zero_vals(prefix_counts) / prefix_total_ngrams\n",
    "        return alpha, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copypaste from NLTK source documentation\n",
    "class KneserNeyInterpolated(InterpolatedLanguageModel):\n",
    "    def __init__(self, order, discount=0.1, **kwargs):\n",
    "        super().__init__(KneserNey, order, params={\"discount\": discount}, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song lyrics</th>\n",
       "      <th>lyr_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nlook i was gonna go easy on you not to hurt ...</td>\n",
       "      <td>[look, i, was, gon, na, go, easy, on, you, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nyou sound like a bitch bitch\\nshut the fuck ...</td>\n",
       "      <td>[you, sound, like, a, bitch, bitch, shut, the,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Song lyrics  \\\n",
       "0  \\nlook i was gonna go easy on you not to hurt ...   \n",
       "1  \\nyou sound like a bitch bitch\\nshut the fuck ...   \n",
       "\n",
       "                                           lyr_token  \n",
       "0  [look, i, was, gon, na, go, easy, on, you, not...  \n",
       "1  [you, sound, like, a, bitch, bitch, shut, the,...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyricsdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricsdf['Song lyrics'] = lyricsdf['Song lyrics'].str.lower()\n",
    "lyricsdf['lyr_token'] = lyricsdf.apply(lambda row: nltk.word_tokenize(row['Song lyrics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(lyricsdf, test_size=0.7, random_state=11)\n",
    "trainset = train['lyr_token'].tolist()\n",
    "testset = test['lyr_token'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4118 items>\n"
     ]
    }
   ],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(2, trainset)\n",
    "lm_bigrams = KneserNeyInterpolated(2, discount=0.1)\n",
    "lm_bigrams.fit(train_data, padded_sents)\n",
    "print(lm_bigrams.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully copypasted from https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sues family fightin and just get the fuckin knife is\n",
      "bye to take em not give out the bowl milk\n",
      "wet pile gettin capped and slap you fuckin—oh and now\n",
      "discriminated against picket signs for all aall aall aall aall\n",
      "dinner plates at and back in whoo ladidadi got bent\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sent(lm_bigrams, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity, are you ok?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для юни-, би- и триграмм перплексия одинаковая :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = test['lyr_token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grams = str(testset)\n",
    "test_bigrams = ngrams(test_grams.split(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_test = []\n",
    "for grams in test_bigrams:\n",
    "    bigrams_test.append(grams)\n",
    "bigrams_test = tuple(bigrams_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перплексия для биграмм 4118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4117.999999956597"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_bigrams.perplexity(bigrams_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Юниграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4116 items>\n"
     ]
    }
   ],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(1, trainset)\n",
    "lm_unigrams = KneserNeyInterpolated(1, discount=0.1)\n",
    "lm_unigrams.fit(train_data, padded_sents)\n",
    "print(lm_unigrams.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grams = str(testset)\n",
    "test_unigrams = ngrams(test_grams.split(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_test = []\n",
    "for grams in test_unigrams:\n",
    "    unigrams_test.append(grams)\n",
    "unigrams_test = tuple(unigrams_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перплексия для юниграмм 4116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4116.000000006382"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_unigrams.perplexity(unigrams_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4118 items>\n"
     ]
    }
   ],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(3, trainset)\n",
    "lm_trigrams = KneserNeyInterpolated(3, discount=0.1)\n",
    "lm_trigrams.fit(train_data, padded_sents)\n",
    "print(lm_trigrams.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grams = str(testset)\n",
    "test_trigrams = ngrams(test_grams.split(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_test = []\n",
    "for grams in test_trigrams:\n",
    "    trigrams_test.append(grams)\n",
    "trigrams_test = tuple(trigrams_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перплексия для триграмм 4118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4117.999999956597"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_trigrams.perplexity(trigrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily based on:\n",
    "    https://www.kaggle.com/alvations/n-gram-language-model-with-nltk and other sources because I am stupid as f.ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
