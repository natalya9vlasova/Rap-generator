{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After each model for different n-grams there is an example of generated text. Unfortunately, all examples leave much to be desired\n",
    "***\n",
    "#### A final table with perplexity values for different models is provided below the application of different models.\n",
    "***\n",
    "#### At the end of the file there is an attempt to use Markov chain and links to sources, which were used in this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Forest_Gump\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "nltk.download('punkt')\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.lm import KneserNeyInterpolated \n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chunk below was commented after the work was done in order not to waste space on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking whether installed module works correctly, setting Eminem as the author\n",
    "genius = lyricsgenius.Genius(\"jHJB8vqrbfVDIGM1KrETFx6s7EG9SGN3RGcmr63z7Gjgz8dYY4LizTL9rGupTAl1\")\n",
    "#at first used to sort by title, but it brings remixes of same song, so let us sort by popularity \n",
    "#artist = genius.search_artist(\"Eminem\", max_songs=400, sort=\"popularity\")\n",
    "#genius.remove_section_headers = True\n",
    "#genius.verbose = False\n",
    "#artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alternate_names', 'api_path', 'description', 'facebook_name', 'followers_count', 'header_image_url', 'id', 'image_url', 'instagram_name', 'is_meme_verified', 'is_verified', 'name', 'translation_artist', 'twitter_name', 'url', 'current_user_metadata', 'iq', 'description_annotation', 'user', 'songs'])\n"
     ]
    }
   ],
   "source": [
    "#open the file and have a look at keys\n",
    "lyrics = 'Lyrics_Eminem.json'\n",
    "with open(lyrics) as eminem_lyrics:\n",
    "    messyvocab = json.load(eminem_lyrics)\n",
    "dict_keys = messyvocab.keys()\n",
    "print(dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracting songs' lyrics and form a vocabulary\n",
    "vocab = messyvocab.get('songs')\n",
    "lyrics_vocab = []\n",
    "for i in range(0,len(vocab)):\n",
    "    index = vocab[i]\n",
    "    song = index.get('lyrics')\n",
    "    lyrics_vocab.append(song)\n",
    "lyricsdf = pd.DataFrame(lyrics_vocab, columns=['Song lyrics'])\n",
    "lyricsdf = lyricsdf.dropna()\n",
    "#clear the punctuation\n",
    "for i in range(0,len(lyricsdf)):\n",
    "    stringlyric = lyricsdf.iloc[i,0]\n",
    "    for x in ['intro', 'verse', 'chorus','!','\"', \"'\", 'â€™', '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','\\\\',']',\n",
    " '^','_','`','{','|','}','~', '1', '...']:   \n",
    "        stringlyric = stringlyric.replace(x, '')\n",
    "        lyricsdf.iloc[i,0] = stringlyric\n",
    "#lowercasing and tokenization \n",
    "lyricsdf['Song lyrics'] = lyricsdf['Song lyrics'].str.lower()\n",
    "lyricsdf['lyr_token'] = lyricsdf.apply(lambda row: nltk.word_tokenize(row['Song lyrics']), axis=1)\n",
    "#forming a list with tokenized lyrics\n",
    "tokenized_text = lyricsdf['lyr_token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Top 20 word in Eminem rap lyrics')"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flattened_list = []\n",
    "for x in tokenized_text:\n",
    "    for y in x:\n",
    "        flattened_list.append(y)\n",
    "word_count = Counter(flattened_list).most_common(20)\n",
    "word_count_df = pd.DataFrame(word_count, columns = ['word', 'count'])\n",
    "\n",
    "#creating a plot, depicting words distribution\n",
    "fig, (ax) = plt.subplots(figsize = (10, 4))\n",
    "sns.barplot(x = word_count_df['word'], y = word_count_df['count'], ax = ax)\n",
    "ax.set_ylabel('count', fontsize = 10)\n",
    "ax.set_xlabel('word',fontsize = 10)\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.set_title('Top 20 word in Eminem rap lyrics', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we know, the bigger is testset, the better, therefore let's set the size of 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(tokenized_text, test_size=0.6, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At first I used only 400 Eminem's songs, and got pretty high perplexity. Then it was decided to expand train set in order to get better perplexity, so below I append to train set the lyrics of 50 Cent's, since the Internet told me they are quite similar to Eminem's. I have searched for and downloaded .json with his lyrics before, so now let's upload some more rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alternate_names', 'api_path', 'description', 'facebook_name', 'followers_count', 'header_image_url', 'id', 'image_url', 'instagram_name', 'is_meme_verified', 'is_verified', 'name', 'translation_artist', 'twitter_name', 'url', 'current_user_metadata', 'iq', 'description_annotation', 'user', 'songs'])\n"
     ]
    }
   ],
   "source": [
    "#same procedure as with Eminem's lyrics, we open the file and look at the keys\n",
    "cent_lyrics = 'Lyrics_50Cent.json'\n",
    "with open(cent_lyrics) as fifty_cent_lyrics:\n",
    "    centmessyvocab = json.load(fifty_cent_lyrics)\n",
    "dict_keys = centmessyvocab.keys()\n",
    "print(dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting songs' lyrics and form a vocabulary\n",
    "vocab = centmessyvocab.get('songs')\n",
    "cent_lyrics_vocab = []\n",
    "for i in range(0,len(vocab)):\n",
    "    index = vocab[i]\n",
    "    song = index.get('lyrics')\n",
    "    cent_lyrics_vocab.append(song)\n",
    "centlyricsdf = pd.DataFrame(cent_lyrics_vocab, columns=['Song lyrics'])\n",
    "centlyricsdf = centlyricsdf.dropna()\n",
    "#clear the punctuation\n",
    "for i in range(0,len(centlyricsdf)):\n",
    "    stringlyric = centlyricsdf.iloc[i,0]\n",
    "    for x in ['intro', 'verse', 'chorus','!','\"', \"'\", 'â€™', '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','\\\\',']',\n",
    " '^','_','`','{','|','}','~', '1', '...']:   \n",
    "        stringlyric = stringlyric.replace(x, '')\n",
    "        centlyricsdf.iloc[i,0] = stringlyric\n",
    "#lowercasing and tokenization \n",
    "centlyricsdf['Song lyrics'] = centlyricsdf['Song lyrics'].str.lower()\n",
    "centlyricsdf['lyr_token'] = centlyricsdf.apply(lambda row: nltk.word_tokenize(row['Song lyrics']), axis=1)\n",
    "#forming a list with tokenized lyrics of 50Cent which will be appended to train set\n",
    "append_to_train_50cent = centlyricsdf['lyr_token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding the train set\n",
    "for x in append_to_train_50cent:\n",
    "    train.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kneser-Ney Interpolated for unigrams, default value of discount = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1_KN, padded_sents_1_KN = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_unigrams_KN = ngrams(test_grams.split(), n)\n",
    "unigrams_KN_test = []\n",
    "for grams in test_unigrams_KN:\n",
    "    unigrams_KN_test.append(grams)\n",
    "unigrams_KN_test = tuple(unigrams_KN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_KN = KneserNeyInterpolated(n, discount=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_KN.fit(train_data_1_KN, padded_sents_1_KN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9394.999999950445"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_KN.perplexity(unigrams_KN_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kneser-Ney Interpolated for unigrams, set value of discount = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_9_KN, padded_sents_9_KN = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_unigrams_KN9 = ngrams(test_grams.split(), n)\n",
    "unigrams_KN9_test = []\n",
    "for grams in test_unigrams_KN9:\n",
    "    unigrams_KN9_test.append(grams)\n",
    "unigrams_KN9_test = tuple(unigrams_KN9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_KN_9 = KneserNeyInterpolated(n, discount=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_KN_9.fit(train_data_9_KN, padded_sents_9_KN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9394.999999950445"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigram_KN_PP = model_1_KN_9.perplexity(unigrams_KN9_test)\n",
    "Unigram_KN_PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bullshit', 'steps', 'sclass', 'dialing', 'klepto', 'hyper', 'pegged', 'shopper', 'birth', 'age']\n"
     ]
    }
   ],
   "source": [
    "print(model_1_KN_9.generate(10, random_seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see above, simple generation shows separate words, therefore, new function is included below\n",
    "#which is fully copypasted from https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station sat heezy dimes learned ha.. shifty entertain jesus monopolize\n",
      "bullshit steps sclass dialing klepto hyper pegged shopper birth age\n",
      "wait vicious attacks bendin sprigs rikers please even newport nicest\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_sent(model_1_KN_9, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace model for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_LA, padded_sents_LA = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_unigrams_LA = ngrams(test_grams.split(), n)\n",
    "unigrams_LA_test = []\n",
    "for grams in test_unigrams_LA:\n",
    "    unigrams_LA_test.append(grams)\n",
    "unigrams_LA_test = tuple(unigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace = Laplace(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace.fit(train_data_LA, padded_sents_LA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154122.00000158613"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It makes me wanna cry too\n",
    "model_laplace.perplexity(unigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigram_laplace_PP = model_laplace.perplexity(unigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip style in from man im thats got lay nigga\n",
      "buttered to ta forth livin its pippen the be a\n",
      "year writin and back this so quick gun o of\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in range(3):\n",
    "    print(generate_sent(model_laplace, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MLE, padded_sents_MLE = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_unigrams_MLE = ngrams(test_grams.split(), n)\n",
    "unigrams_MLE_test = []\n",
    "for grams in test_unigrams_MLE:\n",
    "    unigrams_MLE_test.append(grams)\n",
    "unigrams_MLE_test = tuple(unigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE.fit(train_data_MLE, padded_sents_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unfortunately, it is infinite because of unknowkn n-grams in test set\n",
    "model_MLE.perplexity(unigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigram_mle_PP = model_MLE.perplexity(unigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to take in fuckin man im the gunshot legal nigga\n",
      "buzzed to testarossa fuck long its plain the be a\n",
      "york ya and baby through son rap happy of of\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in range(3):\n",
    "    print(generate_sent(model_MLE, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kneser-Ney Interpolated for bigrams, default value of discount = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1_KN_2, padded_sents_1_KN_2 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_bigrams_KN = ngrams(test_grams.split(), n)\n",
    "bigrams_KN_test = []\n",
    "for grams in test_bigrams_KN:\n",
    "    bigrams_KN_test.append(grams)\n",
    "bigrams_KN_test = tuple(bigrams_KN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_KN = KneserNeyInterpolated(n, discount=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_KN.fit(train_data_1_KN_2, padded_sents_1_KN_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9397.000000039661"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_KN.perplexity(bigrams_KN_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kneser-Ney Interpolated for bigrams, set value of discount = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_9_KN_2, padded_sents_9_KN_2 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_bigrams_KN9 = ngrams(test_grams.split(), n)\n",
    "bigrams_KN9_test = []\n",
    "for grams in test_bigrams_KN9:\n",
    "    bigrams_KN9_test.append(grams)\n",
    "bigrams_KN9_test = tuple(bigrams_KN9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_KN_9 = KneserNeyInterpolated(n, discount=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_KN_9.fit(train_data_9_KN_2, padded_sents_9_KN_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9397.000000039661"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_KN_9.perplexity(bigrams_KN9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigram_KN_PP = model_2_KN_9.perplexity(bigrams_KN9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer rickyyyyyyyyyyyyyyyyyyyyyyyyyyyyy verse man ran up against me you my\n",
      "shutting you in love 50 cent kardinal offishall eeny meany\n",
      "fantasies they ready everybody knows everyone knows like a lie\n",
      "crowned you call rider im doin what i like its\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in [5,6,7,8]:\n",
    "    print(generate_sent(model_2_KN_9, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace model for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_LA_2, padded_sents_LA_2 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_bigrams_LA = ngrams(test_grams.split(), n)\n",
    "bigrams_LA_test = []\n",
    "for grams in test_bigrams_LA:\n",
    "    bigrams_LA_test.append(grams)\n",
    "bigrams_LA_test = tuple(bigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace_2 = Laplace(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace_2.fit(train_data_LA_2, padded_sents_LA_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9397.000000039661"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Really huge\n",
    "model_laplace_2.perplexity(bigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigram_laplace_PP = model_laplace_2.perplexity(bigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the showroom floor now yeah\n",
      "the spot on it . pay her she got that\n",
      "head boy tony and many many calibers here and im\n",
      "em where canibus wannafuck your bowels when it\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in [5,6,7,8]:\n",
    "    print(generate_sent(model_laplace_2, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MLE_2, padded_sents_MLE_2 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_bigrams_MLE = ngrams(test_grams.split(), n)\n",
    "bigrams_MLE_test = []\n",
    "for grams in test_bigrams_MLE:\n",
    "    bigrams_MLE_test.append(grams)\n",
    "bigrams_MLE_test = tuple(bigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE_2 = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE_2.fit(train_data_MLE_2, padded_sents_MLE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLE_2.perplexity(bigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigram_mle_PP=model_MLE_2.perplexity(bigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kid in court throwing money man what goes into my\n",
      "nasty girl louie checkers these rap shit clear off life\n",
      "its most rawest form of pot ill kill whats up\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation via MLE for bigrams\n",
    "for i in [9,10,11]:\n",
    "    print(generate_sent(model_MLE_2, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kneser-Ney Interpolated for trigrams, default value of discount = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1_KN_3, padded_sents_1_KN_3 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_trigrams_KN = ngrams(test_grams.split(), n)\n",
    "trigrams_KN_test = []\n",
    "for grams in test_trigrams_KN:\n",
    "    trigrams_KN_test.append(grams)\n",
    "trigrams_KN_test = tuple(trigrams_KN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_KN = KneserNeyInterpolated(n, discount=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_KN.fit(train_data_1_KN_3, padded_sents_1_KN_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9397.000000039661"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_KN.perplexity(trigrams_KN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram_KN_PP = model_3_KN.perplexity(trigrams_KN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station witness 2 i got moneymoney i got my name\n",
      "bulls out i dont know what you sayin boy you\n",
      "wait wait for doctor to get shit confused i wasnt\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in range(3):\n",
    "    print(generate_sent(model_3_KN, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our previous experience shows us, setting different discounts have no effect, so we will look only at default value of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace model for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_LA_3, padded_sents_LA_3 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_trigrams_LA = ngrams(test_grams.split(), n)\n",
    "trigrams_LA_test = []\n",
    "for grams in test_trigrams_LA:\n",
    "    trigrams_LA_test.append(grams)\n",
    "trigrams_LA_test = tuple(trigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace_3 = Laplace(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_laplace_3.fit(train_data_LA_3, padded_sents_LA_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9397.000000039661"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_laplace_3.perplexity(trigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram_laplace_PP = model_laplace_3.perplexity(trigrams_LA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the sidewalk red with the animal hook 50 cent\n",
      "the spot right now . listen i went from sugar\n",
      "he came home actin like i aint hurting im aight\n",
      "eighties and a nigga bout when you cross me im\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation\n",
    "for i in [5,6,7,8]:\n",
    "    print(generate_sent(model_laplace_3, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MLE_3, padded_sents_MLE_3 = padded_everygram_pipeline(n, train)\n",
    "test_grams = str(test)\n",
    "test_trigrams_MLE = ngrams(test_grams.split(), n)\n",
    "trigrams_MLE_test = []\n",
    "for grams in test_trigrams_MLE:\n",
    "    trigrams_MLE_test.append(grams)\n",
    "trigrams_MLE_test = tuple(trigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE_3 = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLE_3.fit(train_data_MLE_3, padded_sents_MLE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLE_3.perplexity(trigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram_mle_PP = model_MLE_3.perplexity(trigrams_MLE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep it girl you a lot of gats testin macks\n",
      "na hear that chorus shorty you dont have nothing no\n",
      "its most purest most rawest form flow almost flawless most\n"
     ]
    }
   ],
   "source": [
    "#Let's look at examples of text generation via MLE for trigrams\n",
    "for i in [9,10,11]:\n",
    "    print(generate_sent(model_MLE_3, 10, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a table with results for each of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f7746258_7b25_11ea_b2cb_7c7635219d8b  {\n",
       "          border: 4px solid #7a7;\n",
       "          font-family: verdana;\n",
       "          font-family: verdana;\n",
       "          font-size: 120%;\n",
       "    }</style><table id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8b\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >N-grams</th>        <th class=\"col_heading level0 col1\" >Kneser-Ney PP</th>        <th class=\"col_heading level0 col2\" >Laplace PP</th>        <th class=\"col_heading level0 col3\" >MLE PP</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8blevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow0_col0\" class=\"data row0 col0\" >1-gram</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow0_col1\" class=\"data row0 col1\" >9395.000000</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow0_col2\" class=\"data row0 col2\" >154122.000002</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow0_col3\" class=\"data row0 col3\" >inf</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8blevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow1_col0\" class=\"data row1 col0\" >2-gram</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow1_col1\" class=\"data row1 col1\" >9397.000000</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow1_col2\" class=\"data row1 col2\" >9397.000000</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow1_col3\" class=\"data row1 col3\" >inf</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8blevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow2_col0\" class=\"data row2 col0\" >3-gram</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow2_col1\" class=\"data row2 col1\" >9397.000000</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow2_col2\" class=\"data row2 col2\" >9397.000000</td>\n",
       "                        <td id=\"T_f7746258_7b25_11ea_b2cb_7c7635219d8brow2_col3\" class=\"data row2 col3\" >inf</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20133e0c1c8>"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = {'N-grams':['1-gram', '2-gram', '3-gram'],\n",
    "        'Kneser-Ney PP': [Unigram_KN_PP,Bigram_KN_PP,Trigram_KN_PP],\n",
    "        'Laplace PP': [Unigram_laplace_PP,Bigram_laplace_PP,Trigram_laplace_PP],\n",
    "        'MLE PP': [Unigram_mle_PP,Bigram_mle_PP,Trigram_mle_PP] }\n",
    "models = pd.DataFrame(compare, columns = ['N-grams', 'Kneser-Ney PP', 'Laplace PP', 'MLE PP'])\n",
    "models_perplexity = models.style.set_table_styles([{'selector':'','props':[('border','4px solid #7a7'), ('font-family', 'verdana'),('font-family', 'verdana'), ('font-size', '120%')]}])\n",
    "models_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">*Conclusion*</span>: perplexity, which indicates the quality of the model, is huge. Therefore, all used models are not that good (perhaps because of my mistakes). Tuning of discount in Kneser-Ney model did not make any changes in perplexity, which is strangely equal for different n-grams. Moreover, even after train set expansion, perplexity values did not become satisfactory. And, in general,  generated texts hardly resemble rap lyrics (unless the rapper is drunk), and look more like a set of random obscene words\n",
    "\n",
    "I understand that there must me a lot of mistakes, but the whole task was quite cool thing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below there is an attempt to use principle of Markov chains for lyrics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = lyricsdf['lyr_token'].tolist()\n",
    "flattened_lyr = []\n",
    "for x in tokenized_text:\n",
    "    for y in x:\n",
    "        flattened_lyr.append(y)\n",
    "\n",
    "def make_bigrams(text):\n",
    "    for i in range(len(text)-1):\n",
    "        yield (text[i], text[i+1])\n",
    "        \n",
    "bigrams = make_bigrams(flattened_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for word_1, word_2 in bigrams:\n",
    "    if word_1 in word_dict.keys():\n",
    "        word_dict[word_1].append(word_2)\n",
    "    else:\n",
    "        word_dict[word_1] = [word_2]\n",
    "        \n",
    "first_word = np.random.choice(flattened_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 met this fucking chick is to hurt you cave in\n",
      "the pot make us together everything is on riding\n",
      "on it trying to run deep as i had them some twins\n",
      "before they say that i need somethin to outrun me\n",
      "to toe from amityville hell stab in the panelin\n",
      "cussin no paper i know where must go to think of\n",
      "evil as i apologize if i have no help propel me\n",
      "only women skinning your mom your verdict while i\n",
      "get to dust and did you pissed me and if ten\n",
      "freaky girls man ya sweeter than an mc\n"
     ]
    }
   ],
   "source": [
    "while first_word.islower():\n",
    "    first_word = np.random.choice(flattened_lyr)\n",
    "\n",
    "chain = [first_word]\n",
    "n_words = 100\n",
    "\n",
    "for i in range(n_words):\n",
    "    chain.append(np.random.choice(word_dict[chain[-1]]))\n",
    "\n",
    "generated_text = ' '.join(chain)\n",
    "rap = textwrap.fill(generated_text, width=50)\n",
    " \n",
    "print(rap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunately, I have no clue how to evaluate the quality of text generation, provided by this method. As far as I can judge, lyrics are not that bad, since there are many pairs and triples of words commonly used together, and some lines even make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it turned out, there occurs a fault when perplexity is counted, so either the other version of nltk should be installed, or it can be done as follows. I have updated my nltk version, but sometimes perplexity was still counted with mistakes, so I decided to include the source code below and use it when the error appears. \n",
    "(https://www.nltk.org/_modules/nltk/lm/api.html#LanguageModel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.models import LanguageModel\n",
    "#copypaste from NLTK source documentation\n",
    "class InterpolatedLanguageModel(LanguageModel):\n",
    "    def __init__(self, smoothing_cls, order, **kwargs):\n",
    "        assert issubclass(smoothing_cls, Smoothing)\n",
    "        params = kwargs.pop(\"params\", {})\n",
    "        super().__init__(order, **kwargs)\n",
    "        self.estimator = smoothing_cls(self.vocab, self.counts, **params)\n",
    "\n",
    "\n",
    "    def unmasked_score(self, word, context=None):\n",
    "        if not context:\n",
    "                return self.estimator.unigram_score(word)\n",
    "        if not self.counts[context]:\n",
    "             return self.unmasked_score(word, context[1:])\n",
    "        alpha, gamma = self.estimator.alpha_gamma(word, context)\n",
    "        return alpha + gamma * self.unmasked_score(word, context[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_non_zero_vals(dictionary):\n",
    "    return sum(1.0 for c in dictionary.values() if c > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is KneserNey from NLTK documentation but with some amendments from https://github.com/nltk/nltk/pull/2363/commits/ce74e449dc9526e19596b1c4a9c510bbb35812cc \n",
    "from nltk.lm.models import Smoothing\n",
    "class KneserNey(Smoothing):\n",
    "    def __init__(self, vocabulary, counter, discount=0.1, **kwargs):\n",
    "        super(KneserNey, self).__init__(vocabulary, counter, *kwargs)\n",
    "        super().__init__(vocabulary, counter, **kwargs)\n",
    "        self.discount = discount\n",
    "\n",
    "    def unigram_score(self, word):\n",
    "        return 1.0 / len(self.vocab)\n",
    "\n",
    "    def alpha_gamma(self, word, context):\n",
    "        prefix_counts = self.counts[context]\n",
    "        prefix_total_ngrams = prefix_counts.N()\n",
    "        alpha = max(prefix_counts[word] - self.discount, 0.0) / prefix_total_ngrams\n",
    "        gamma = self.discount * _count_non_zero_vals(prefix_counts) / prefix_total_ngrams\n",
    "        return alpha, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copypaste from NLTK source documentation\n",
    "class KneserNeyInterpolated(InterpolatedLanguageModel):\n",
    "    def __init__(self, order, discount=0.1, **kwargs):\n",
    "        super().__init__(KneserNey, order, params={\"discount\": discount}, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily based on:\n",
    "    https://www.kaggle.com/alvations/n-gram-language-model-with-nltk and https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
